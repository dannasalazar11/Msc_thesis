{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6395ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:55.121193Z",
     "iopub.status.busy": "2025-04-29T16:16:55.120607Z",
     "iopub.status.idle": "2025-04-29T16:16:55.142998Z",
     "shell.execute_reply": "2025-04-29T16:16:55.141930Z"
    },
    "papermill": {
     "duration": 0.030178,
     "end_time": "2025-04-29T16:16:55.144758",
     "exception": false,
     "start_time": "2025-04-29T16:16:55.114580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef5c1ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:55.154092Z",
     "iopub.status.busy": "2025-04-29T16:16:55.153707Z",
     "iopub.status.idle": "2025-04-29T16:16:58.027100Z",
     "shell.execute_reply": "2025-04-29T16:16:58.026199Z"
    },
    "papermill": {
     "duration": 2.880061,
     "end_time": "2025-04-29T16:16:58.029036",
     "exception": false,
     "start_time": "2025-04-29T16:16:55.148975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ruta_carpeta_TDAH = '/kaggle/input/ieee-tdah-control-database/ieee/ADHD_group'  \n",
    "ruta_carpeta_control = '/kaggle/input/ieee-tdah-control-database/ieee/Control_group'  \n",
    "\n",
    "# Nombre de cada sujeto\n",
    "sujetos_TDAH = [archivo[:-4] for archivo in os.listdir(ruta_carpeta_TDAH) if archivo.endswith('.mat')]\n",
    "sujetos_TDAH.pop()\n",
    "sujetos_control = [archivo[:-4] for archivo in os.listdir(ruta_carpeta_control) if archivo.endswith('.mat')]\n",
    "\n",
    "diagnostico = {}\n",
    "\n",
    "for sbj in sujetos_TDAH:\n",
    "    diagnostico[sbj] = 1\n",
    "\n",
    "for sbj in sujetos_control:\n",
    "    diagnostico[sbj] = 0\n",
    "\n",
    "# organizamos los datos de los sujetos con TDAH en un diccionario\n",
    "eeg_tdah = {}\n",
    "\n",
    "for i in range(len(sujetos_TDAH)):\n",
    "    sbj = sujetos_TDAH[i]\n",
    "    mat_file_path = ruta_carpeta_TDAH+'/'+sbj+'.mat'\n",
    "    data = scipy.io.loadmat(mat_file_path)\n",
    "    columna = list(data.keys())[-1]\n",
    "    eeg_tdah[sbj] = data[columna].T\n",
    "\n",
    "# organizamos los datos de los sujetos de control en un diccionario\n",
    "eeg_control = {}\n",
    "\n",
    "for i in range(len(sujetos_control)):\n",
    "    sbj = sujetos_control[i]\n",
    "    mat_file_path = ruta_carpeta_control+'/'+sbj+'.mat'\n",
    "    data = scipy.io.loadmat(mat_file_path)\n",
    "    columna = list(data.keys())[-1]\n",
    "    eeg_control[sbj] = data[columna].T\n",
    "\n",
    "def segmentar_senales(db, labels):\n",
    "    \"\"\"\n",
    "    Divide las se√±ales EEG en segmentos de 512 instantes con un traslape del 50%.\n",
    "    \n",
    "    Args:\n",
    "        db (dict): Diccionario donde las claves son los nombres de los sujetos y los valores\n",
    "                   son matrices de forma CxT_i (C = canales, T_i = tiempo).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nuevo diccionario con los segmentos de cada sujeto.\n",
    "    \"\"\"\n",
    "    segmentos_db = {}\n",
    "    segmento_tamano = 512\n",
    "    paso = int(segmento_tamano * 0.5)  # Traslape del 50%\n",
    "    i = 0\n",
    "    \n",
    "    segmentos = []\n",
    "    y = []\n",
    "    sbjs = []\n",
    "    \n",
    "    for sujeto, senal in db.items():\n",
    "        C, T = senal.shape\n",
    "        \n",
    "        # Crear segmentos con traslape\n",
    "        for inicio in range(0, T - segmento_tamano + 1, paso):\n",
    "            segmento = senal[:, inicio:inicio + segmento_tamano]\n",
    "            segmentos.append(segmento)\n",
    "            y.append(labels[i])\n",
    "            sbjs.append(sujeto)\n",
    "\n",
    "        i += 1\n",
    "    return np.array(segmentos), np.array(y), sbjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8441deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:58.038926Z",
     "iopub.status.busy": "2025-04-29T16:16:58.038436Z",
     "iopub.status.idle": "2025-04-29T16:16:58.043741Z",
     "shell.execute_reply": "2025-04-29T16:16:58.042632Z"
    },
    "papermill": {
     "duration": 0.012331,
     "end_time": "2025-04-29T16:16:58.045729",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.033398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data= eeg_control | eeg_tdah\n",
    "zeros = np.zeros(len(eeg_control))\n",
    "ones = np.ones(len(eeg_tdah))\n",
    "y = np.hstack((zeros, ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f34963",
   "metadata": {
    "papermill": {
     "duration": 0.003571,
     "end_time": "2025-04-29T16:16:58.053525",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.049954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **To build the proposal**\n",
    "\n",
    "## **Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45428ff2",
   "metadata": {
    "papermill": {
     "duration": 0.003691,
     "end_time": "2025-04-29T16:16:58.061112",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.057421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **1.Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c23d8b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:58.069999Z",
     "iopub.status.busy": "2025-04-29T16:16:58.069559Z",
     "iopub.status.idle": "2025-04-29T16:16:58.960207Z",
     "shell.execute_reply": "2025-04-29T16:16:58.959247Z"
    },
    "papermill": {
     "duration": 0.897134,
     "end_time": "2025-04-29T16:16:58.962025",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.064891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Bandpass filter (0.5-63 Hz)\n",
    "def bandpass_filter(signal, lowcut=0.5, highcut=63, fs=128, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    b, a = butter(order, [lowcut/nyquist, highcut/nyquist], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# Notch filter at 50 Hz\n",
    "def notch_filter(signal, fs=128, freq=50.0):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = (freq - 1) / nyquist\n",
    "    high = (freq + 1) / nyquist\n",
    "    b, a = butter(2, [low, high], btype='bandstop')\n",
    "    return filtfilt(b, a, signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc318b39",
   "metadata": {
    "papermill": {
     "duration": 0.003528,
     "end_time": "2025-04-29T16:16:58.969597",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.966069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **2. Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8398f597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:58.978564Z",
     "iopub.status.busy": "2025-04-29T16:16:58.978129Z",
     "iopub.status.idle": "2025-04-29T16:16:58.984146Z",
     "shell.execute_reply": "2025-04-29T16:16:58.983262Z"
    },
    "papermill": {
     "duration": 0.012296,
     "end_time": "2025-04-29T16:16:58.985769",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.973473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_signal(signal, segment_length=512, overlap=256):\n",
    "    segments = []\n",
    "    for i in range(0, len(signal) - segment_length + 1, overlap):\n",
    "        segments.append(signal[i:i+segment_length])\n",
    "    return segments\n",
    "\n",
    "# Define sub-band ranges\n",
    "subbands = {\n",
    "    \"delta\": (0.5, 4),\n",
    "    \"theta\": (4, 8),\n",
    "    \"alpha\": (8, 13),\n",
    "    \"beta\": (13, 30)\n",
    "}\n",
    "\n",
    "def extract_subbands(segment, fs=128):\n",
    "    return {band: bandpass_filter(segment, low, high, fs=fs) \n",
    "            for band, (low, high) in subbands.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b77000",
   "metadata": {
    "papermill": {
     "duration": 0.003476,
     "end_time": "2025-04-29T16:16:58.993316",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.989840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **3. Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98379056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:59.002194Z",
     "iopub.status.busy": "2025-04-29T16:16:59.001773Z",
     "iopub.status.idle": "2025-04-29T16:16:59.009773Z",
     "shell.execute_reply": "2025-04-29T16:16:59.008880Z"
    },
    "papermill": {
     "duration": 0.014388,
     "end_time": "2025-04-29T16:16:59.011472",
     "exception": false,
     "start_time": "2025-04-29T16:16:58.997084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "\n",
    "def shannon_entropy(signal, bins=100):\n",
    "    hist, _ = np.histogram(signal, bins=bins, density=True)\n",
    "    hist = hist[hist > 0]  # Avoid log(0)\n",
    "    return -np.sum(hist * np.log2(hist))\n",
    "\n",
    "def compute_features(segment):\n",
    "    features = {}\n",
    "    features['std'] = np.std(segment)\n",
    "    features['rms'] = np.sqrt(np.mean(np.square(segment)))\n",
    "    features['skew'] = skew(segment)\n",
    "    features['kurt'] = kurtosis(segment)\n",
    "\n",
    "    # Hjorth parameters\n",
    "    diff1 = np.diff(segment)\n",
    "    diff2 = np.diff(diff1)\n",
    "    features['hjorth_activity'] = np.var(segment)\n",
    "    features['hjorth_mobility'] = np.sqrt(np.var(diff1) / np.var(segment))\n",
    "    features['hjorth_complexity'] = np.sqrt(np.var(diff2) / np.var(diff1)) / features['hjorth_mobility']\n",
    "\n",
    "    # Shannon entropy (custom)\n",
    "    features['shannon'] = shannon_entropy(segment)\n",
    "\n",
    "    # Spectral entropy\n",
    "    freqs, psd = welch(segment, fs=128)\n",
    "    psd_norm = psd / np.sum(psd)\n",
    "    features['spectral_entropy'] = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))\n",
    "\n",
    "    # Band power\n",
    "    features['band_power'] = np.sum(psd)\n",
    "\n",
    "    # Power Spectral Density - Average value\n",
    "    features['psd_avg'] = np.mean(psd)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a3d105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:16:59.020952Z",
     "iopub.status.busy": "2025-04-29T16:16:59.020448Z",
     "iopub.status.idle": "2025-04-29T16:42:51.679652Z",
     "shell.execute_reply": "2025-04-29T16:42:51.678415Z"
    },
    "papermill": {
     "duration": 1552.666096,
     "end_time": "2025-04-29T16:42:51.681671",
     "exception": false,
     "start_time": "2025-04-29T16:16:59.015575",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject 1: v306. Label: 0.0\n",
      "Processing subject 2: v46p. Label: 0.0\n",
      "Processing subject 3: v140. Label: 0.0\n",
      "Processing subject 4: v108. Label: 0.0\n",
      "Processing subject 5: v107. Label: 0.0\n",
      "Processing subject 6: v110. Label: 0.0\n",
      "Processing subject 7: v143. Label: 0.0\n",
      "Processing subject 8: v307. Label: 0.0\n",
      "Processing subject 9: v308. Label: 0.0\n",
      "Processing subject 10: v42p. Label: 0.0\n",
      "Processing subject 11: v56p. Label: 0.0\n",
      "Processing subject 12: v114. Label: 0.0\n",
      "Processing subject 13: v120. Label: 0.0\n",
      "Processing subject 14: v305. Label: 0.0\n",
      "Processing subject 15: v149. Label: 0.0\n",
      "Processing subject 16: v113. Label: 0.0\n",
      "Processing subject 17: v303. Label: 0.0\n",
      "Processing subject 18: v297. Label: 0.0\n",
      "Processing subject 19: v47p. Label: 0.0\n",
      "Processing subject 20: v54p. Label: 0.0\n",
      "Processing subject 21: v59p. Label: 0.0\n",
      "Processing subject 22: v57p. Label: 0.0\n",
      "Processing subject 23: v58p. Label: 0.0\n",
      "Processing subject 24: v115. Label: 0.0\n",
      "Processing subject 25: v117. Label: 0.0\n",
      "Processing subject 26: v118. Label: 0.0\n",
      "Processing subject 27: v116. Label: 0.0\n",
      "Processing subject 28: v50p. Label: 0.0\n",
      "Processing subject 29: v125. Label: 0.0\n",
      "Processing subject 30: v49p. Label: 0.0\n",
      "Processing subject 31: v111. Label: 0.0\n",
      "Processing subject 32: v302. Label: 0.0\n",
      "Processing subject 33: v109. Label: 0.0\n",
      "Processing subject 34: v48p. Label: 0.0\n",
      "Processing subject 35: v123. Label: 0.0\n",
      "Processing subject 36: v298. Label: 0.0\n",
      "Processing subject 37: v60p. Label: 0.0\n",
      "Processing subject 38: v43p. Label: 0.0\n",
      "Processing subject 39: v51p. Label: 0.0\n",
      "Processing subject 40: v45p. Label: 0.0\n",
      "Processing subject 41: v131. Label: 0.0\n",
      "Processing subject 42: v138. Label: 0.0\n",
      "Processing subject 43: v55p. Label: 0.0\n",
      "Processing subject 44: v127. Label: 0.0\n",
      "Processing subject 45: v53p. Label: 0.0\n",
      "Processing subject 46: v147. Label: 0.0\n",
      "Processing subject 47: v151. Label: 0.0\n",
      "Processing subject 48: v310. Label: 0.0\n",
      "Processing subject 49: v44p. Label: 0.0\n",
      "Processing subject 50: v41p. Label: 0.0\n",
      "Processing subject 51: v112. Label: 0.0\n",
      "Processing subject 52: v134. Label: 0.0\n",
      "Processing subject 53: v52p. Label: 0.0\n",
      "Processing subject 54: v121. Label: 0.0\n",
      "Processing subject 55: v129. Label: 0.0\n",
      "Processing subject 56: v133. Label: 0.0\n",
      "Processing subject 57: v304. Label: 0.0\n",
      "Processing subject 58: v299. Label: 0.0\n",
      "Processing subject 59: v309. Label: 0.0\n",
      "Processing subject 60: v300. Label: 0.0\n",
      "Processing subject 61: v34p. Label: 1.0\n",
      "Processing subject 62: v33p. Label: 1.0\n",
      "Processing subject 63: v30p. Label: 1.0\n",
      "Processing subject 64: v22p. Label: 1.0\n",
      "Processing subject 65: v35p. Label: 1.0\n",
      "Processing subject 66: v27p. Label: 1.0\n",
      "Processing subject 67: v254. Label: 1.0\n",
      "Processing subject 68: v238. Label: 1.0\n",
      "Processing subject 69: v15p. Label: 1.0\n",
      "Processing subject 70: v183. Label: 1.0\n",
      "Processing subject 71: v6p. Label: 1.0\n",
      "Processing subject 72: v190. Label: 1.0\n",
      "Processing subject 73: v227. Label: 1.0\n",
      "Processing subject 74: v270. Label: 1.0\n",
      "Processing subject 75: v14p. Label: 1.0\n",
      "Processing subject 76: v1p. Label: 1.0\n",
      "Processing subject 77: v204. Label: 1.0\n",
      "Processing subject 78: v206. Label: 1.0\n",
      "Processing subject 79: v234. Label: 1.0\n",
      "Processing subject 80: v177. Label: 1.0\n",
      "Processing subject 81: v265. Label: 1.0\n",
      "Processing subject 82: v25p. Label: 1.0\n",
      "Processing subject 83: v274. Label: 1.0\n",
      "Processing subject 84: v40p. Label: 1.0\n",
      "Processing subject 85: v213. Label: 1.0\n",
      "Processing subject 86: v181. Label: 1.0\n",
      "Processing subject 87: v31p. Label: 1.0\n",
      "Processing subject 88: v37p. Label: 1.0\n",
      "Processing subject 89: v18p. Label: 1.0\n",
      "Processing subject 90: v29p. Label: 1.0\n",
      "Processing subject 91: v12p. Label: 1.0\n",
      "Processing subject 92: v244. Label: 1.0\n",
      "Processing subject 93: v28p. Label: 1.0\n",
      "Processing subject 94: v250. Label: 1.0\n",
      "Processing subject 95: v21p. Label: 1.0\n",
      "Processing subject 96: v200. Label: 1.0\n",
      "Processing subject 97: v231. Label: 1.0\n",
      "Processing subject 98: v39p. Label: 1.0\n",
      "Processing subject 99: v209. Label: 1.0\n",
      "Processing subject 100: v236. Label: 1.0\n",
      "Processing subject 101: v246. Label: 1.0\n",
      "Processing subject 102: v3p. Label: 1.0\n",
      "Processing subject 103: v10p. Label: 1.0\n",
      "Processing subject 104: v32p. Label: 1.0\n",
      "Processing subject 105: v173. Label: 1.0\n",
      "Processing subject 106: v196. Label: 1.0\n",
      "Processing subject 107: v215. Label: 1.0\n",
      "Processing subject 108: v286. Label: 1.0\n",
      "Processing subject 109: v20p. Label: 1.0\n",
      "Processing subject 110: v38p. Label: 1.0\n",
      "Processing subject 111: v284. Label: 1.0\n",
      "Processing subject 112: v263. Label: 1.0\n",
      "Processing subject 113: v24p. Label: 1.0\n",
      "Processing subject 114: v19p. Label: 1.0\n",
      "Processing subject 115: v179. Label: 1.0\n",
      "Processing subject 116: v288. Label: 1.0\n",
      "Processing subject 117: v279. Label: 1.0\n",
      "Processing subject 118: v8p. Label: 1.0\n",
      "Processing subject 119: v198. Label: 1.0\n",
      "Processing subject 120: v219. Label: 1.0\n",
      "X shape: (8213, 836)\n",
      "y shape: (8213,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "labels = []\n",
    "sbjs = []\n",
    "\n",
    "for subj_idx, (subj_id, signals) in enumerate(data.items()):\n",
    "    label = y[subj_idx]\n",
    "    print(f\"Processing subject {subj_idx+1}: {subj_id}. Label: {label}\")\n",
    "\n",
    "    # signals assumed shape: (19 channels, time)\n",
    "    channel_segments = [segment_signal(notch_filter(bandpass_filter(channel_signal)))\n",
    "                        for channel_signal in signals]\n",
    "\n",
    "    # Number of segments per channel might differ slightly due to length‚Äîsync by taking the minimum\n",
    "    num_segments = min(len(segs) for segs in channel_segments)\n",
    "\n",
    "    for i in range(num_segments):  # Loop over segment index\n",
    "        segment_features = []\n",
    "        for ch in range(len(signals)):  # For each channel\n",
    "            seg = channel_segments[ch][i]\n",
    "            subband_signals = extract_subbands(seg)\n",
    "            for band_name in ['delta', 'theta', 'alpha', 'beta']:  # fixed order\n",
    "                filtered = subband_signals[band_name]\n",
    "                feats = compute_features(filtered)\n",
    "                segment_features.extend(list(feats.values()))  # 11 features\n",
    "\n",
    "        X.append(segment_features)  # length should be 4 * 19 * 11 = 836\n",
    "        labels.append(label)\n",
    "        sbjs.append(subj_id)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437a9266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:42:51.705409Z",
     "iopub.status.busy": "2025-04-29T16:42:51.704854Z",
     "iopub.status.idle": "2025-04-29T16:42:51.709272Z",
     "shell.execute_reply": "2025-04-29T16:42:51.708210Z"
    },
    "papermill": {
     "duration": 0.018445,
     "end_time": "2025-04-29T16:42:51.710859",
     "exception": false,
     "start_time": "2025-04-29T16:42:51.692414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deber√≠a quedar de tama√±o segments x 4 x 19 x 11, o sea, cada segmento tiene 836  caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b48bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:42:51.732196Z",
     "iopub.status.busy": "2025-04-29T16:42:51.731729Z",
     "iopub.status.idle": "2025-04-29T16:42:51.833214Z",
     "shell.execute_reply": "2025-04-29T16:42:51.832254Z"
    },
    "papermill": {
     "duration": 0.114354,
     "end_time": "2025-04-29T16:42:51.835318",
     "exception": false,
     "start_time": "2025-04-29T16:42:51.720964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open('y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aae47d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:42:51.857232Z",
     "iopub.status.busy": "2025-04-29T16:42:51.856841Z",
     "iopub.status.idle": "2025-04-29T16:42:52.361879Z",
     "shell.execute_reply": "2025-04-29T16:42:52.360624Z"
    },
    "papermill": {
     "duration": 0.517855,
     "end_time": "2025-04-29T16:42:52.363632",
     "exception": false,
     "start_time": "2025-04-29T16:42:51.845777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 756\n",
      "X shape after ANOVA filter: (8213, 756)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Run ANOVA\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get p-values and scores\n",
    "p_values = selector.pvalues_\n",
    "scores = selector.scores_\n",
    "\n",
    "# Create mask for p-values <= 0.5\n",
    "mask = p_values <= 0.5\n",
    "\n",
    "# Apply the mask to X\n",
    "X_selected = X[:, mask]\n",
    "\n",
    "print(\"Selected features:\", np.sum(mask))\n",
    "print(\"X shape after ANOVA filter:\", X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10f81b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:42:52.385453Z",
     "iopub.status.busy": "2025-04-29T16:42:52.385108Z",
     "iopub.status.idle": "2025-04-29T16:42:53.407047Z",
     "shell.execute_reply": "2025-04-29T16:42:53.405886Z"
    },
    "papermill": {
     "duration": 1.034912,
     "end_time": "2025-04-29T16:42:53.408887",
     "exception": false,
     "start_time": "2025-04-29T16:42:52.373975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ANOVA-selected shape: (8213, 756)\n",
      "Reduced shape after PCA: (8213, 177)\n",
      "Number of components selected: 177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.90)  # Retain 90% of the variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print results\n",
    "print(\"Original ANOVA-selected shape:\", X_selected.shape)\n",
    "print(\"Reduced shape after PCA:\", X_pca.shape)\n",
    "print(\"Number of components selected:\", X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9140f962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T16:42:53.430829Z",
     "iopub.status.busy": "2025-04-29T16:42:53.430444Z",
     "iopub.status.idle": "2025-04-29T16:43:16.771159Z",
     "shell.execute_reply": "2025-04-29T16:43:16.769884Z"
    },
    "papermill": {
     "duration": 23.354126,
     "end_time": "2025-04-29T16:43:16.773457",
     "exception": false,
     "start_time": "2025-04-29T16:42:53.419331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Test subjects: ['v28p', 'v274', 'v1p', 'v231', 'v22p', 'v29p', 'v206', 'v238', 'v31p', 'v35p', 'v177', 'v200', 'v112', 'v113', 'v48p', 'v140', 'v131', 'v125', 'v55p', 'v143', 'v43p', 'v305', 'v134', 'v114']\n",
      "Fold Accuracy: 0.6692\n",
      "Fold 2 - Test subjects: ['v18p', 'v39p', 'v234', 'v32p', 'v190', 'v6p', 'v254', 'v204', 'v24p', 'v183', 'v246', 'v219', 'v298', 'v41p', 'v47p', 'v308', 'v52p', 'v300', 'v59p', 'v299', 'v302', 'v51p', 'v109', 'v127']\n",
      "Fold Accuracy: 0.6643\n",
      "Fold 3 - Test subjects: ['v215', 'v3p', 'v209', 'v37p', 'v213', 'v15p', 'v284', 'v181', 'v19p', 'v34p', 'v263', 'v244', 'v138', 'v121', 'v46p', 'v54p', 'v120', 'v310', 'v147', 'v50p', 'v56p', 'v107', 'v297', 'v108']\n",
      "Fold Accuracy: 0.6431\n",
      "Fold 4 - Test subjects: ['v227', 'v8p', 'v236', 'v14p', 'v196', 'v27p', 'v33p', 'v179', 'v173', 'v10p', 'v265', 'v20p', 'v57p', 'v45p', 'v111', 'v115', 'v53p', 'v118', 'v123', 'v44p', 'v149', 'v303', 'v116', 'v151']\n",
      "Fold Accuracy: 0.7167\n",
      "Fold 5 - Test subjects: ['v279', 'v30p', 'v288', 'v286', 'v250', 'v12p', 'v38p', 'v25p', 'v21p', 'v40p', 'v198', 'v270', 'v117', 'v306', 'v309', 'v110', 'v42p', 'v58p', 'v307', 'v133', 'v304', 'v129', 'v49p', 'v60p']\n",
      "Fold Accuracy: 0.6301\n",
      "\n",
      "‚úÖ Mean Accuracy: 0.6647 ¬± 0.0296\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Asumiendo que ya tienes:\n",
    "# - X_pca: caracter√≠sticas reducidas (n_samples, n_features)\n",
    "# - y: labels (n_samples,)\n",
    "# - sbjs: lista de sujetos para cada segmento (n_samples,)\n",
    "# - folds: lista de folds con sujetos de train y test\n",
    "#   Ejemplo: folds = [(train_subjects, test_subjects), ...]\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/ieee-tdah-control-database/folds.pkl\", \"rb\") as f:\n",
    "    folds = pickle.load(f)\n",
    "    \n",
    "# Preparar listas para resultados\n",
    "scores = []\n",
    "subject_accuracy = {sbj: [] for sbj in set(sbjs)}\n",
    "\n",
    "# üîÅ Loop sobre los folds\n",
    "for fold_idx, (train_subjects, test_subjects) in enumerate(folds):\n",
    "    print(f\"Fold {fold_idx + 1} - Test subjects: {test_subjects}\")\n",
    "\n",
    "    # Obtener √≠ndices por sujeto\n",
    "    train_idx = [i for i, sbj in enumerate(sbjs) if sbj in train_subjects]\n",
    "    test_idx = [i for i, sbj in enumerate(sbjs) if sbj in test_subjects]\n",
    "\n",
    "    # Dividir datos\n",
    "    X_train, X_test = X_pca[train_idx], X_pca[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    sbjs_test = np.array(sbjs)[test_idx]\n",
    "\n",
    "    # Entrenar el modelo SVM (RBF kernel como en el paper)\n",
    "    clf = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar el fold\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores.append(acc)\n",
    "    print(f'Fold Accuracy: {acc:.4f}')\n",
    "\n",
    "    # Evaluar precisi√≥n por sujeto individual\n",
    "    for sbj in test_subjects:\n",
    "        sbj_indices = [i for i, s in enumerate(sbjs_test) if s == sbj]\n",
    "        X_sbj = X_test[sbj_indices]\n",
    "        y_sbj = y_test[sbj_indices]\n",
    "        y_pred_sbj = clf.predict(X_sbj)\n",
    "        acc_sbj = accuracy_score(y_sbj, y_pred_sbj)\n",
    "        subject_accuracy[sbj].append(acc_sbj)\n",
    "\n",
    "# Resultado final\n",
    "mean_acc = np.mean(scores)\n",
    "std_acc = np.std(scores)\n",
    "print(f'\\n‚úÖ Mean Accuracy: {mean_acc:.4f} ¬± {std_acc:.4f}')\n",
    "\n",
    "# Guardar resultados si se desea\n",
    "with open('svm_lsso_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(scores, f)\n",
    "\n",
    "with open('svm_lsso_subject_accuracy.pkl', 'wb') as f:\n",
    "    pickle.dump(subject_accuracy, f)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6567861,
     "sourceId": 10680378,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1585.675932,
   "end_time": "2025-04-29T16:43:17.510071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-29T16:16:51.834139",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
